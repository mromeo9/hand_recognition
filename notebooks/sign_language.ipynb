{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0.1</th>\n",
       "      <th>-0.0609375</th>\n",
       "      <th>-0.029166666666666667</th>\n",
       "      <th>-0.1125</th>\n",
       "      <th>-0.10416666666666667</th>\n",
       "      <th>-0.1453125</th>\n",
       "      <th>-0.175</th>\n",
       "      <th>-0.1703125</th>\n",
       "      <th>...</th>\n",
       "      <th>-0.0015625</th>\n",
       "      <th>-0.13541666666666666.1</th>\n",
       "      <th>0.0453125</th>\n",
       "      <th>-0.16875</th>\n",
       "      <th>0.0328125</th>\n",
       "      <th>-0.21666666666666667</th>\n",
       "      <th>0.028125</th>\n",
       "      <th>-0.16041666666666668</th>\n",
       "      <th>0.0328125.1</th>\n",
       "      <th>-0.14166666666666666</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.059375</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>-0.106250</td>\n",
       "      <td>-0.108333</td>\n",
       "      <td>-0.137500</td>\n",
       "      <td>-0.177083</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>-0.139583</td>\n",
       "      <td>0.051562</td>\n",
       "      <td>-0.170833</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>-0.164583</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>-0.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.060937</td>\n",
       "      <td>-0.029167</td>\n",
       "      <td>-0.110937</td>\n",
       "      <td>-0.108333</td>\n",
       "      <td>-0.143750</td>\n",
       "      <td>-0.181250</td>\n",
       "      <td>-0.170313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.034375</td>\n",
       "      <td>-0.218750</td>\n",
       "      <td>0.029687</td>\n",
       "      <td>-0.164583</td>\n",
       "      <td>0.035937</td>\n",
       "      <td>-0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.060937</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>-0.109375</td>\n",
       "      <td>-0.110417</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.177083</td>\n",
       "      <td>-0.167187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.168750</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>-0.164583</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>-0.147917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.060937</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>-0.109375</td>\n",
       "      <td>-0.110417</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.177083</td>\n",
       "      <td>-0.167187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.168750</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>0.032813</td>\n",
       "      <td>-0.164583</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>-0.147917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.059375</td>\n",
       "      <td>-0.029167</td>\n",
       "      <td>-0.109375</td>\n",
       "      <td>-0.108333</td>\n",
       "      <td>-0.140625</td>\n",
       "      <td>-0.179167</td>\n",
       "      <td>-0.167187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.137500</td>\n",
       "      <td>0.048438</td>\n",
       "      <td>-0.170833</td>\n",
       "      <td>0.035937</td>\n",
       "      <td>-0.216667</td>\n",
       "      <td>0.029687</td>\n",
       "      <td>-0.162500</td>\n",
       "      <td>0.035937</td>\n",
       "      <td>-0.143750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  0.0  0.0.1  -0.0609375  -0.029166666666666667   -0.1125  \\\n",
       "0  1  0.0    0.0   -0.059375              -0.031250 -0.106250   \n",
       "1  1  0.0    0.0   -0.060937              -0.029167 -0.110937   \n",
       "2  1  0.0    0.0   -0.060937              -0.031250 -0.109375   \n",
       "3  1  0.0    0.0   -0.060937              -0.031250 -0.109375   \n",
       "4  1  0.0    0.0   -0.059375              -0.029167 -0.109375   \n",
       "\n",
       "   -0.10416666666666667  -0.1453125    -0.175  -0.1703125  ...  -0.0015625  \\\n",
       "0             -0.108333   -0.137500 -0.177083   -0.164062  ...    0.004687   \n",
       "1             -0.108333   -0.143750 -0.181250   -0.170313  ...    0.000000   \n",
       "2             -0.110417   -0.140625 -0.177083   -0.167187  ...    0.003125   \n",
       "3             -0.110417   -0.140625 -0.177083   -0.167187  ...    0.003125   \n",
       "4             -0.108333   -0.140625 -0.179167   -0.167187  ...    0.000000   \n",
       "\n",
       "   -0.13541666666666666.1  0.0453125  -0.16875  0.0328125  \\\n",
       "0               -0.139583   0.051562 -0.170833   0.037500   \n",
       "1               -0.141667   0.046875 -0.175000   0.034375   \n",
       "2               -0.141667   0.050000 -0.168750   0.037500   \n",
       "3               -0.141667   0.050000 -0.168750   0.037500   \n",
       "4               -0.137500   0.048438 -0.170833   0.035937   \n",
       "\n",
       "   -0.21666666666666667  0.028125  -0.16041666666666668  0.0328125.1  \\\n",
       "0             -0.216667  0.032813             -0.164583     0.039062   \n",
       "1             -0.218750  0.029687             -0.164583     0.035937   \n",
       "2             -0.216667  0.032813             -0.164583     0.037500   \n",
       "3             -0.216667  0.032813             -0.164583     0.037500   \n",
       "4             -0.216667  0.029687             -0.162500     0.035937   \n",
       "\n",
       "   -0.14166666666666666  \n",
       "0             -0.145833  \n",
       "1             -0.150000  \n",
       "2             -0.147917  \n",
       "3             -0.147917  \n",
       "4             -0.143750  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "[ 0.          0.         -0.0609375  -0.03125    -0.109375   -0.10833333\n",
      " -0.140625   -0.17916667 -0.1671875  -0.22291667 -0.075      -0.17916667\n",
      " -0.096875   -0.22916667 -0.0828125  -0.1625     -0.06875    -0.1375\n",
      " -0.0359375  -0.18541667 -0.0546875  -0.2375     -0.04375    -0.15625\n",
      " -0.0328125  -0.13958333  0.00625    -0.18333333 -0.0078125  -0.24583333\n",
      " -0.0046875  -0.16458333  0.003125   -0.14166667  0.0484375  -0.17083333\n",
      "  0.0375     -0.22083333  0.0328125  -0.16875     0.0375     -0.14791667]\n",
      "[ 1  1  1 ... 26 26 26]\n"
     ]
    }
   ],
   "source": [
    "print(type(df.iloc[0,1]))\n",
    "X = np.array(df.iloc[:,1:].values)\n",
    "Y = np.array(df.iloc[:,0].values)\n",
    "print(X[10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Y)):\n",
    "    if Y[i] <10:\n",
    "        Y[i] = Y[i] - 1\n",
    "    else: Y[i] = Y[i] -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0 ... 24 24 24]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.         -0.084375   -0.06041667 -0.140625   -0.17708333\n",
      " -0.1671875  -0.27291667 -0.1875     -0.34583333 -0.0890625  -0.29375\n",
      " -0.11875    -0.35416667 -0.115625   -0.25833333 -0.1        -0.20208333\n",
      " -0.034375   -0.29583333 -0.0671875  -0.33958333 -0.0640625  -0.225\n",
      " -0.0484375  -0.18333333  0.021875   -0.28125    -0.00625    -0.32083333\n",
      " -0.009375   -0.21041667  0.003125   -0.15833333  0.078125   -0.26041667\n",
      "  0.078125   -0.37916667  0.06875    -0.44583333  0.0625     -0.51458333]\n",
      "[ 8 18 22 ... 10  1  1]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, features = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "labels = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U'\n",
    "          'V', 'W', 'X', 'Y', 'Z')\n",
    "\n",
    "output_size = 25\n",
    "\n",
    "print(np.min(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu',input_shape=(42,)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss = SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.any(np.isnan(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_23 (Dense)            (None, 256)               11008     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 42)                5418      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131,626\n",
      "Trainable params: 131,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "220/220 [==============================] - 1s 4ms/step - loss: 1.6954 - accuracy: 0.4512 - val_loss: 0.7924 - val_accuracy: 0.7096\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.7491 - val_loss: 0.4660 - val_accuracy: 0.8189\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.4044 - accuracy: 0.8496 - val_loss: 0.3546 - val_accuracy: 0.8508\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.3184 - accuracy: 0.8784 - val_loss: 0.2691 - val_accuracy: 0.9112\n",
      "Epoch 5/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.2501 - accuracy: 0.9100 - val_loss: 0.2281 - val_accuracy: 0.9294\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.2380 - accuracy: 0.9134 - val_loss: 0.2003 - val_accuracy: 0.9180\n",
      "Epoch 7/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1655 - accuracy: 0.9399 - val_loss: 0.1364 - val_accuracy: 0.9487\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1359 - accuracy: 0.9467 - val_loss: 0.1583 - val_accuracy: 0.9522\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1851 - accuracy: 0.9399 - val_loss: 0.2893 - val_accuracy: 0.8884\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1193 - accuracy: 0.9610 - val_loss: 0.1483 - val_accuracy: 0.9408\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1020 - accuracy: 0.9635 - val_loss: 0.0705 - val_accuracy: 0.9829\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1019 - accuracy: 0.9644 - val_loss: 0.1207 - val_accuracy: 0.9715\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0891 - accuracy: 0.9661 - val_loss: 0.1035 - val_accuracy: 0.9658\n",
      "Epoch 14/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0761 - accuracy: 0.9724 - val_loss: 0.0866 - val_accuracy: 0.9761\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0868 - accuracy: 0.9698 - val_loss: 0.1247 - val_accuracy: 0.9658\n",
      "Epoch 16/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0588 - accuracy: 0.9789 - val_loss: 0.1389 - val_accuracy: 0.9533\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0892 - accuracy: 0.9707 - val_loss: 0.0899 - val_accuracy: 0.9738\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0854 - accuracy: 0.9690 - val_loss: 0.0904 - val_accuracy: 0.9761\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0580 - accuracy: 0.9806 - val_loss: 0.0908 - val_accuracy: 0.9715\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9852 - val_loss: 0.0672 - val_accuracy: 0.9818\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1139 - accuracy: 0.9670 - val_loss: 0.1170 - val_accuracy: 0.9544\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0623 - accuracy: 0.9784 - val_loss: 0.0519 - val_accuracy: 0.9886\n",
      "Epoch 23/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0464 - accuracy: 0.9823 - val_loss: 0.0797 - val_accuracy: 0.9829\n",
      "Epoch 24/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0412 - accuracy: 0.9852 - val_loss: 0.0597 - val_accuracy: 0.9841\n",
      "Epoch 25/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0338 - accuracy: 0.9892 - val_loss: 0.0537 - val_accuracy: 0.9875\n",
      "Epoch 26/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1133 - accuracy: 0.9670 - val_loss: 0.1594 - val_accuracy: 0.9590\n",
      "Epoch 27/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0422 - accuracy: 0.9878 - val_loss: 0.1271 - val_accuracy: 0.9533\n",
      "Epoch 28/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0204 - accuracy: 0.9940 - val_loss: 0.0513 - val_accuracy: 0.9875\n",
      "Epoch 29/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0397 - accuracy: 0.9860 - val_loss: 0.0604 - val_accuracy: 0.9806\n",
      "Epoch 30/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0792 - accuracy: 0.9766 - val_loss: 0.1219 - val_accuracy: 0.9624\n",
      "Epoch 31/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0675 - accuracy: 0.9766 - val_loss: 0.0401 - val_accuracy: 0.9932\n",
      "Epoch 32/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0431 - accuracy: 0.9860 - val_loss: 0.0439 - val_accuracy: 0.9875\n",
      "Epoch 33/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0240 - accuracy: 0.9909 - val_loss: 0.0489 - val_accuracy: 0.9897\n",
      "Epoch 34/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0236 - accuracy: 0.9912 - val_loss: 0.0420 - val_accuracy: 0.9897\n",
      "Epoch 35/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.1038 - accuracy: 0.9684 - val_loss: 0.0652 - val_accuracy: 0.9818\n",
      "Epoch 36/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 0.0456 - val_accuracy: 0.9909\n",
      "Epoch 37/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0203 - accuracy: 0.9949 - val_loss: 0.0726 - val_accuracy: 0.9806\n",
      "Epoch 38/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0489 - accuracy: 0.9846 - val_loss: 0.0367 - val_accuracy: 0.9954\n",
      "Epoch 39/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0283 - accuracy: 0.9906 - val_loss: 0.0539 - val_accuracy: 0.9875\n",
      "Epoch 40/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0358 - accuracy: 0.9915 - val_loss: 0.0476 - val_accuracy: 0.9920\n",
      "Epoch 41/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 0.0744 - val_accuracy: 0.9806\n",
      "Epoch 42/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.0648 - val_accuracy: 0.9852\n",
      "Epoch 43/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0427 - accuracy: 0.9860 - val_loss: 0.0330 - val_accuracy: 0.9943\n",
      "Epoch 44/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0816 - accuracy: 0.9758 - val_loss: 0.0940 - val_accuracy: 0.9704\n",
      "Epoch 45/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0205 - accuracy: 0.9943 - val_loss: 0.0435 - val_accuracy: 0.9897\n",
      "Epoch 46/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9934 - val_loss: 0.0340 - val_accuracy: 0.9932\n",
      "Epoch 47/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0422 - accuracy: 0.9858 - val_loss: 0.0721 - val_accuracy: 0.9806\n",
      "Epoch 48/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0450 - accuracy: 0.9863 - val_loss: 0.1084 - val_accuracy: 0.9647\n",
      "Epoch 49/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9932 - val_loss: 0.0278 - val_accuracy: 0.9932\n",
      "Epoch 50/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0295 - accuracy: 0.9909 - val_loss: 0.0260 - val_accuracy: 0.9920\n",
      "Epoch 51/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0120 - accuracy: 0.9963 - val_loss: 0.0249 - val_accuracy: 0.9954\n",
      "Epoch 52/100\n",
      "220/220 [==============================] - 1s 2ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.0284 - val_accuracy: 0.9932\n",
      "Epoch 53/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0455 - accuracy: 0.9849 - val_loss: 0.0464 - val_accuracy: 0.9886\n",
      "Epoch 54/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0197 - accuracy: 0.9943 - val_loss: 0.2872 - val_accuracy: 0.9100\n",
      "Epoch 55/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0364 - accuracy: 0.9889 - val_loss: 0.0218 - val_accuracy: 0.9954\n",
      "Epoch 56/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.0879 - val_accuracy: 0.9738\n",
      "Epoch 57/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0288 - accuracy: 0.9920 - val_loss: 0.0219 - val_accuracy: 0.9954\n",
      "Epoch 58/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0167 - accuracy: 0.9960 - val_loss: 0.0303 - val_accuracy: 0.9920\n",
      "Epoch 59/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9946 - val_loss: 0.0527 - val_accuracy: 0.9852\n",
      "Epoch 60/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0488 - accuracy: 0.9869 - val_loss: 0.0651 - val_accuracy: 0.9897\n",
      "Epoch 61/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0387 - accuracy: 0.9875 - val_loss: 0.0402 - val_accuracy: 0.9920\n",
      "Epoch 62/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9943 - val_loss: 0.0232 - val_accuracy: 0.9954\n",
      "Epoch 63/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0260 - val_accuracy: 0.9943\n",
      "Epoch 64/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0378 - accuracy: 0.9923 - val_loss: 0.0516 - val_accuracy: 0.9863\n",
      "Epoch 65/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0284 - accuracy: 0.9920 - val_loss: 0.0438 - val_accuracy: 0.9920\n",
      "Epoch 66/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9878 - val_loss: 0.0468 - val_accuracy: 0.9909\n",
      "Epoch 67/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0129 - accuracy: 0.9952 - val_loss: 0.0360 - val_accuracy: 0.9920\n",
      "Epoch 68/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0411 - accuracy: 0.9860 - val_loss: 0.0995 - val_accuracy: 0.9727\n",
      "Epoch 69/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0164 - accuracy: 0.9940 - val_loss: 0.0330 - val_accuracy: 0.9920\n",
      "Epoch 70/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 0.0277 - val_accuracy: 0.9943\n",
      "Epoch 71/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.0268 - val_accuracy: 0.9943\n",
      "Epoch 72/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0281 - val_accuracy: 0.9943\n",
      "Epoch 73/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0377 - accuracy: 0.9886 - val_loss: 0.0273 - val_accuracy: 0.9932\n",
      "Epoch 74/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.0326 - val_accuracy: 0.9943\n",
      "Epoch 75/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9957 - val_loss: 0.0419 - val_accuracy: 0.9909\n",
      "Epoch 76/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0283 - accuracy: 0.9929 - val_loss: 0.0454 - val_accuracy: 0.9909\n",
      "Epoch 77/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0260 - accuracy: 0.9920 - val_loss: 0.0604 - val_accuracy: 0.9829\n",
      "Epoch 78/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0271 - accuracy: 0.9903 - val_loss: 0.0369 - val_accuracy: 0.9920\n",
      "Epoch 79/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9983 - val_loss: 0.0321 - val_accuracy: 0.9943\n",
      "Epoch 80/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 0.9932 - val_loss: 0.1935 - val_accuracy: 0.9544\n",
      "Epoch 81/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0762 - accuracy: 0.9812 - val_loss: 0.0504 - val_accuracy: 0.9852\n",
      "Epoch 82/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 0.0370 - val_accuracy: 0.9909\n",
      "Epoch 83/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9934 - val_loss: 0.0586 - val_accuracy: 0.9795\n",
      "Epoch 84/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0279 - val_accuracy: 0.9943\n",
      "Epoch 85/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0667 - val_accuracy: 0.9829\n",
      "Epoch 86/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9926 - val_loss: 0.0464 - val_accuracy: 0.9920\n",
      "Epoch 87/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0616 - accuracy: 0.9832 - val_loss: 0.0376 - val_accuracy: 0.9909\n",
      "Epoch 88/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0284 - val_accuracy: 0.9954\n",
      "Epoch 89/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0139 - accuracy: 0.9960 - val_loss: 0.0259 - val_accuracy: 0.9943\n",
      "Epoch 90/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.0402 - val_accuracy: 0.9897\n",
      "Epoch 91/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.0366 - val_accuracy: 0.9932\n",
      "Epoch 92/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0060 - accuracy: 0.9974 - val_loss: 0.0373 - val_accuracy: 0.9932\n",
      "Epoch 93/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 0.9952 - val_loss: 0.0501 - val_accuracy: 0.9875\n",
      "Epoch 94/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.0359 - val_accuracy: 0.9852\n",
      "Epoch 95/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.0342 - val_accuracy: 0.9909\n",
      "Epoch 96/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0290 - val_accuracy: 0.9943\n",
      "Epoch 97/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 6.0099e-04 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 0.9932\n",
      "Epoch 98/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0294 - accuracy: 0.9934 - val_loss: 0.4185 - val_accuracy: 0.9362\n",
      "Epoch 99/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 0.1582 - val_accuracy: 0.9544\n",
      "Epoch 100/100\n",
      "220/220 [==============================] - 1s 3ms/step - loss: 0.0178 - accuracy: 0.9946 - val_loss: 0.0359 - val_accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "training_data = model.fit(x_train,y_train,epochs=100,batch_size=16,shuffle=True, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sign_language_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

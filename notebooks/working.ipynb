{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Create a way to access the camera and show the hand\n",
    "2. Using media pipe show the hands and the landmarks given \n",
    "3. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow('img',frame)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv.imshow('webcam', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "\n",
    "print(isinstance(x,int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Media Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2 as cv\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "cap = cv.VideoCapture(1)\n",
    "ret, frame = cap.read()\n",
    "cv.imshow('img',frame)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    frame = cv.flip(frame, 1)\n",
    "    results = hands.process(frame)\n",
    "    \n",
    "    image = cv.cvtColor(frame, cv.COLOR_RGB2BGR)\n",
    "\n",
    "    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "    mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.imshow('MediaPipe Hands', image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hand_landmarks in results.multi_hand_landmarks:\n",
    "    print(hand_landmarks.landmark[0].x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [np.array((1,2))]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bounding_box(image, landmarks):\n",
    "    H,W = image.shape[:2]\n",
    "    landmark_array = np.empty((0, 2), np.int16)\n",
    "\n",
    "    for landmark in landmarks.landmark:\n",
    "\n",
    "        landmark_x = min(int(landmark.x*W),W-1)\n",
    "        landmark_y = min(int(landmark.y*H),H-1)\n",
    "        landmark_point = [np.array((landmark_x,landmark_y))]\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(image, landmark_array):\n",
    "     \n",
    "     x, y, w, h = cv.boundingRect(landmark_array)\n",
    "     cv.rectangle(image, (x, y), (x+w, y+h),\n",
    "                     (0, 0, 0), 2)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fingers(image, landmark_array, z_position):\n",
    "\n",
    "    def_rad = 25\n",
    "    max_r = 5\n",
    "    for i, landmark in enumerate(landmark_array):\n",
    "        current_r = min(max_r, int(z_position[i]*def_rad))\n",
    "        cv.rectangle(image, (landmark[0]-current_r, landmark[1]-current_r),\n",
    "                     (landmark[0]+current_r, landmark[1]+current_r), (0,0,0), -1)\n",
    "\n",
    "    #Thumb\n",
    "    cv.line(image, landmark_array[0], landmark_array[1], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[1], landmark_array[2], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[2], landmark_array[3], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[3], landmark_array[4], (0,0,0),1)\n",
    "\n",
    "    #pointer\n",
    "    cv.line(image, landmark_array[0], landmark_array[5], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[5], landmark_array[6], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[6], landmark_array[7], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[7], landmark_array[8], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[5], landmark_array[9], (0,0,0),1)\n",
    "\n",
    "    #middle\n",
    "    cv.line(image, landmark_array[9], landmark_array[10], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[10], landmark_array[11], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[11], landmark_array[12], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[9], landmark_array[13], (0,0,0),1)\n",
    "\n",
    "    #ring\n",
    "    cv.line(image, landmark_array[13], landmark_array[14], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[14], landmark_array[15], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[15], landmark_array[16], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[13], landmark_array[17], (0,0,0),1)\n",
    "\n",
    "    #pinkie\n",
    "    cv.line(image, landmark_array[0], landmark_array[17], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[17], landmark_array[18], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[18], landmark_array[19], (0,0,0),1)\n",
    "    cv.line(image, landmark_array[19], landmark_array[20], (0,0,0),1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using hands.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n\u001b[0;32m      3\u001b[0m hands \u001b[38;5;241m=\u001b[39m mp_hands\u001b[38;5;241m.\u001b[39mHands(\n\u001b[0;32m      4\u001b[0m     static_image_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     max_num_hands\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m      6\u001b[0m     min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv.VideoCapture(1)\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # copy image for running\n",
    "    image = cv.flip(image, 1)\n",
    "    debug_image = np.copy(image)\n",
    "\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    if results.multi_hand_landmarks is not None:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                    debug_image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        # Find the Bounding box\n",
    "        bbox = find_bounding_box(image, hand_landmarks)\n",
    "        cv.rectangle(debug_image, (bbox[0], bbox[1]), (bbox[2], bbox[3]),\n",
    "                     (0, 0, 0), 3)\n",
    "        \n",
    "    cv.imshow('MediaPipe Hands', debug_image)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gesture recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_landmark_array(image, landmarks):\n",
    "    H,W = image.shape[:2]\n",
    "    landmark_array = np.empty((0, 2), np.int16)\n",
    "    z_position = []\n",
    "    for landmark in landmarks:\n",
    "\n",
    "        landmark_x = min(int(landmark.x*W),W-1)\n",
    "        landmark_y = min(int(landmark.y*H),H-1)\n",
    "        landmark_point = [np.array((landmark_x,landmark_y))]\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "        z_position.append(landmark.z)\n",
    "    \n",
    "    return landmark_array, z_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(image, gesture, landmark_array, handedness):\n",
    "\n",
    "    # Text generation\n",
    "    font = cv.FONT_HERSHEY_SIMPLEX\n",
    "    category = gesture.category_name\n",
    "    if handedness.category_name == 'Right':\n",
    "        hand = 'Left'\n",
    "    elif handedness.category_name == 'Left':\n",
    "        hand = 'Right'\n",
    "        \n",
    "    text = hand + ' | ' + category\n",
    "\n",
    "    #Positon of text box   \n",
    "    x, y, _, _ = cv.boundingRect(landmark_array)\n",
    "    tx_offset = x\n",
    "    ty_offset = y-5\n",
    "\n",
    "    #Text Size \n",
    "    (t_w, t_h) = cv.getTextSize(text, font, fontScale=0.5, thickness=1)[0]\n",
    "    \n",
    "    # Background for the text \n",
    "    box_coords = ((tx_offset - 2, ty_offset + 2), (tx_offset + t_w + 2, ty_offset - t_h - 2))\n",
    "    cv.rectangle(image, box_coords[0], box_coords[1], (0,0,0), cv.FILLED)\n",
    "\n",
    "    # PLace the text\n",
    "    cv.putText(image, text, (tx_offset,ty_offset), font, 0.5, (255,255,255), lineType=cv.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\romeo\\OneDrive\\Documents\\2024\\Personal\\hand_recognition\\venv\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "cap = cv.VideoCapture(1)\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "model_path = './gesture_recognizer.task'\n",
    "\n",
    "# Create a gesture recognizer instance with the image mode:\n",
    "with open(model_path, 'rb') as file:\n",
    "    model_data = file.read()\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_buffer=model_data),\n",
    "    running_mode=VisionRunningMode.IMAGE,\n",
    "    num_hands = 2)\n",
    "\n",
    "recognizer = GestureRecognizer.create_from_options(options)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # copy image for running\n",
    "    image = cv.flip(image,1)\n",
    "    debug_image = np.copy(image)\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    # recognise the landmarks \n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "    gesture_recognition_result = recognizer.recognize(mp_image)\n",
    "\n",
    "    if gesture_recognition_result.hand_landmarks:\n",
    "        \n",
    "        multi_hand_landmarks = gesture_recognition_result.hand_landmarks\n",
    "        multi_gestures = gesture_recognition_result.gestures\n",
    "        multi_handedness = gesture_recognition_result.handedness\n",
    "\n",
    "        for hand_landmark, gestures, handedness in zip(multi_hand_landmarks,multi_gestures, multi_handedness):\n",
    "            #Set the landmarks into an array\n",
    "            landmark_array, z_position = make_landmark_array(image, hand_landmark)\n",
    "\n",
    "            #Draw a bounding box around he hand\n",
    "            draw_bounding_box(debug_image, landmark_array)\n",
    "\n",
    "            #Add the landmarks for the image\n",
    "            draw_fingers(debug_image, landmark_array, z_position)\n",
    "\n",
    "            #Add text for the image\n",
    "            add_text(debug_image, gestures[0], landmark_array, handedness[0])\n",
    "\n",
    "    cv.imshow('Camera', debug_image)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.imshow('image', image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H,W = image.shape[:2]\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "model_path = './gesture_recognizer.task'\n",
    "\n",
    "# Create a gesture recognizer instance with the image mode:\n",
    "with open(model_path, 'rb') as file:\n",
    "    model_data = file.read()\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_buffer=model_data),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "recognizer = GestureRecognizer.create_from_options(options)\n",
    "\n",
    "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "gesture_recognition_result = recognizer.recognize(mp_image)\n",
    "\n",
    "landmark_list = []\n",
    "z = []\n",
    "for i, landmark in enumerate(gesture_recognition_result.hand_landmarks[0]):\n",
    "    landmark_x = min(int(landmark.x*W),W-1)\n",
    "    landmark_y = min(int(landmark.y*H),H-1)\n",
    "    z.append(landmark.z)\n",
    "    landmark_list.append((landmark_x,landmark_y))\n",
    "\n",
    "\n",
    "print(gesture_recognition_result.handedness[0][0].category_name)\n",
    "if gesture_recognition_result.handedness[0][0].category_name == 'Left':\n",
    "    hand = 'Right'\n",
    "elif gesture_recognition_result.handedness[0][0].category_name == 'Right':\n",
    "    hand = \"Left\"\n",
    "r = 20\n",
    "d2 = np.copy(debug_image)\n",
    "\n",
    "x, y, w, h = cv.boundingRect(np.array(landmark_list))\n",
    "font = cv.FONT_HERSHEY_SIMPLEX\n",
    "text = hand + ' | ' + gesture_recognition_result.gestures[0][0].category_name\n",
    "(t_w, t_h) = cv.getTextSize(text, font, fontScale=0.5, thickness=1)[0]\n",
    "tx_offset = x\n",
    "ty_offset = y-5\n",
    "box_coords = ((tx_offset - 2, ty_offset + 2), (tx_offset + t_w + 2, ty_offset - t_h - 2))\n",
    "cv.rectangle(d2, box_coords[0], box_coords[1], (0,0,0), cv.FILLED)\n",
    "\n",
    "cv.putText(d2, text, (tx_offset,ty_offset), font, 0.5, (255,255,255), lineType=cv.LINE_AA)\n",
    "\n",
    "\n",
    "for i, landmark in enumerate(landmark_list):\n",
    "    cv.rectangle(d2, (landmark[0]-int(r*z[i]), landmark[1]-int(r*z[i])), (landmark[0]+int(r*z[i]), landmark[1]+int(r*z[i])), (0,0,0), -1)\n",
    "\n",
    "#Thumb\n",
    "cv.line(d2, landmark_list[0], landmark_list[1], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[1], landmark_list[2], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[2], landmark_list[3], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[3], landmark_list[4], (0,0,0),1)\n",
    "\n",
    "#pointer\n",
    "cv.line(d2, landmark_list[0], landmark_list[5], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[5], landmark_list[6], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[6], landmark_list[7], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[7], landmark_list[8], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[5], landmark_list[9], (0,0,0),1)\n",
    "\n",
    "#middle\n",
    "cv.line(d2, landmark_list[9], landmark_list[10], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[10], landmark_list[11], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[11], landmark_list[12], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[9], landmark_list[13], (0,0,0),1)\n",
    "\n",
    "#ring\n",
    "cv.line(d2, landmark_list[13], landmark_list[14], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[14], landmark_list[15], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[15], landmark_list[16], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[13], landmark_list[17], (0,0,0),1)\n",
    "\n",
    "#pinkie\n",
    "cv.line(d2, landmark_list[0], landmark_list[17], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[17], landmark_list[18], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[18], landmark_list[19], (0,0,0),1)\n",
    "cv.line(d2, landmark_list[19], landmark_list[20], (0,0,0),1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv.imshow('image', d2)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0: 'unknown',\n",
    "    1: 'closed_fist',\n",
    "    2: 'Open_palm',\n",
    "    3: 'pointing_up',\n",
    "    4: 'thumb_down',\n",
    "    5: 'Thumb_up',\n",
    "    6: 'victory',\n",
    "    7: 'ILoveYou'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
